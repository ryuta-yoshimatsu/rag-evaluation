{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a87a4cd-8a01-4e35-8a71-eaf91ed4ddd2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# LLM Evaluation with MLflow example\n",
    "\n",
    "This notebook demonstrates how to evaluate various LLMs and RAG systems with MLflow, leveraging simple metrics such as perplexity and toxicity, as well as LLM-judged metrics such as relevance, and even custom LLM-judged metrics such as professionalism.\n",
    "\n",
    "For details about how to use `mlflow.evaluate()`, refer to Evaluate LLMs with MLflow ([AWS](https://docs.databricks.com/en/mlflow/llm-evaluate.html)|[Azure](https://learn.microsoft.com/azure/databricks/mlflow/llm-evaluate)).\n",
    "\n",
    "## Requirements\n",
    " \n",
    "To use the MLflow LLM evaluation feature, you must use MLflow flavor 2.8.0 or above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82acd20e-d12b-43dc-b91a-34254b98ebde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If your cluster is running Databricks Runtime, uncomment and run the following cell to install the `mlflow` library. This is required for Databricks Runtime clusters only. If you are using a cluster running Databricks Runtime ML, skip to Set OpenAI Key step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23217f2b-421f-4db0-8cc9-94df0eb7ed08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If you are running Databricks Runtime, uncomment this line and run this cell:\n",
    "#%pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aef0015-bb7a-4b35-9407-de7996e1ebd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941f74bd-47de-4dda-a89f-fd5d1b9cc213",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.9.2'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "\n",
    "## Check yout MLflow version\n",
    "mlflow.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cce6412a-2279-4ec1-a344-fa76fec70ee1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Set OpenAI Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bec25067-224d-4ee8-9b5d-0beeb6cde684",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(scope=\"\", key=\"\")\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"\"\n",
    "os.environ[\"OPENAI_DEPLOYMENT_NAME\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9bbfc03-793e-4b95-b009-ef30dccd7e7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Question-Answering Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff253b9e-59e8-40e0-92d8-8f9ef85348fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a test case of `inputs` that is passed into the model and `ground_truth` which is used to compare against the generated output from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6199fb3f-5951-42fe-891a-2227010b630a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"How does useEffect() work?\",\n",
    "            \"What does the static keyword in a function mean?\",\n",
    "            \"What does the 'finally' block in Python do?\",\n",
    "            \"What is the difference between multiprocessing and multithreading?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"The useEffect() hook tells React that your component needs to do something after render. React will remember the function you passed (we’ll refer to it as our “effect”), and call it later after performing the DOM updates.\",\n",
    "            \"Static members belongs to the class, rather than a specific instance. This means that only one instance of a static member exists, even if you create multiple objects of the class, or if you don't create any. It will be shared by all objects.\",\n",
    "            \"'Finally' defines a block of code to run when the try... except...else block is final. The finally block will be executed no matter if the try block raises an error or not.\",\n",
    "            \"Multithreading refers to the ability of a processor to execute multiple threads concurrently, where each thread runs a process. Whereas multiprocessing refers to the ability of a system to run multiple processors in parallel, where each processor can run one or more threads.\",\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06825224-49bd-452d-8dab-b11ca8130017",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a simple OpenAI model that asks gpt-3.5 to answer the question in two sentences. Call `mlflow.evaluate()` with the model and evaluation dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b67eb6f-c91a-4f9a-ac0d-01fd22b087c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 12:25:31 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.9.2/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ca9be26b2d411c8e43fe427abeda4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad235a05ccd4a86acf3536471f2eb35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 12:25:37 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2024/03/15 12:25:37 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n2024/03/15 12:25:38 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n2024/03/15 12:25:38 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n2024/03/15 12:25:38 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n2024/03/15 12:25:38 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n2024/03/15 12:25:38 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n2024/03/15 12:25:39 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n2024/03/15 12:25:39 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n2024/03/15 12:25:39 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n2024/03/15 12:25:39 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n2024/03/15 12:25:39 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'toxicity/v1/mean': 0.00020113740538363345,\n",
       " 'toxicity/v1/variance': 1.06491546679447e-09,\n",
       " 'toxicity/v1/p90': 0.00023621094587724655,\n",
       " 'toxicity/v1/ratio': 0.0,\n",
       " 'exact_match/v1': 0.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Answer the following question in two sentences\"\n",
    "    basic_qa_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        task=openai.ChatCompletion,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        targets=\"ground_truth\",  # specify which column corresponds to the expected output\n",
    "        model_type=\"question-answering\",  # model type indicates which metrics are relevant for this task\n",
    "        evaluators=\"default\",\n",
    "    )\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d078816-1de1-4a6e-b757-5c9cbe056638",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Inspect the evaluation results table as a dataframe to see row-by-row metrics to further assess model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28688e6c-6a2d-40bd-a737-58cfe70f2e10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753773a1712c408b8381b19781c089ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>inputs</th><th>ground_truth</th><th>outputs</th><th>token_count</th><th>toxicity/v1/score</th></tr></thead><tbody><tr><td>How does useEffect() work?</td><td>The useEffect() hook tells React that your component needs to do something after render. React will remember the function you passed (we’ll refer to it as our “effect”), and call it later after performing the DOM updates.</td><td>useEffect() is a hook in React that allows you to run side-effects such as fetching data or updating the DOM after every render. It takes two arguments: a function that contains the side effect code and an optional array of dependencies that tells React when to re-run the effect.</td><td>56</td><td>2.1918650000000002E-4</td></tr><tr><td>What does the static keyword in a function mean?</td><td>Static members belongs to the class, rather than a specific instance. This means that only one instance of a static member exists, even if you create multiple objects of the class, or if you don't create any. It will be shared by all objects.</td><td>The static keyword in a function means that the function retains the value of its local variables across multiple function calls. This also means that the function is only visible inside the file it is declared in.</td><td>39</td><td>1.584084E-4</td></tr><tr><td>What does the 'finally' block in Python do?</td><td>'Finally' defines a block of code to run when the try... except...else block is final. The finally block will be executed no matter if the try block raises an error or not.</td><td>The 'finally' block in Python is used to specify a block of code to be executed after the try and except blocks, regardless of whether an exception was raised or not. It is commonly used to clean up resources such as closing files or releasing network connections.</td><td>52</td><td>2.4350710000000002E-4</td></tr><tr><td>What is the difference between multiprocessing and multithreading?</td><td>Multithreading refers to the ability of a processor to execute multiple threads concurrently, where each thread runs a process. Whereas multiprocessing refers to the ability of a system to run multiple processors in parallel, where each processor can run one or more threads.</td><td>Multiprocessing involves the execution of multiple processes simultaneously, utilizing multiple processors. On the other hand, multithreading involves the execution of multiple threads within a single process, utilizing a single processor.</td><td>38</td><td>1.834476E-4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "How does useEffect() work?",
         "The useEffect() hook tells React that your component needs to do something after render. React will remember the function you passed (we’ll refer to it as our “effect”), and call it later after performing the DOM updates.",
         "useEffect() is a hook in React that allows you to run side-effects such as fetching data or updating the DOM after every render. It takes two arguments: a function that contains the side effect code and an optional array of dependencies that tells React when to re-run the effect.",
         56,
         2.1918650000000002E-4
        ],
        [
         "What does the static keyword in a function mean?",
         "Static members belongs to the class, rather than a specific instance. This means that only one instance of a static member exists, even if you create multiple objects of the class, or if you don't create any. It will be shared by all objects.",
         "The static keyword in a function means that the function retains the value of its local variables across multiple function calls. This also means that the function is only visible inside the file it is declared in.",
         39,
         1.584084E-4
        ],
        [
         "What does the 'finally' block in Python do?",
         "'Finally' defines a block of code to run when the try... except...else block is final. The finally block will be executed no matter if the try block raises an error or not.",
         "The 'finally' block in Python is used to specify a block of code to be executed after the try and except blocks, regardless of whether an exception was raised or not. It is commonly used to clean up resources such as closing files or releasing network connections.",
         52,
         2.4350710000000002E-4
        ],
        [
         "What is the difference between multiprocessing and multithreading?",
         "Multithreading refers to the ability of a processor to execute multiple threads concurrently, where each thread runs a process. Whereas multiprocessing refers to the ability of a system to run multiple processors in parallel, where each processor can run one or more threads.",
         "Multiprocessing involves the execution of multiple processes simultaneously, utilizing multiple processors. On the other hand, multithreading involves the execution of multiple threads within a single process, utilizing a single processor.",
         38,
         1.834476E-4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "inputs",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ground_truth",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "outputs",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "token_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "toxicity/v1/score",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results.tables[\"eval_results_table\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7363c9-3b73-4e3f-bf7c-1d6887fb4f9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## LLM-judged correctness with OpenAI GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd23fe79-cfbf-42a7-a3f3-14badfe20db5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Construct an answer similarity metric using the `answer_similarity()` metric  function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b35b52-5b8f-4b72-9de8-fec05f01e722",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=answer_similarity, greater_is_better=True, long_name=answer_similarity, version=v1, metric_details=\nTask:\nYou must return the following fields in your response in two lines, one below the other:\nscore: Your numerical score for the model's answer_similarity based on the rubric\njustification: Your reasoning about the model's answer_similarity score\n\nYou are an impartial judge. You will be given an input that was sent to a machine\nlearning model, and you will be given an output that the model produced. You\nmay also be given additional information that was used by the model to generate the output.\n\nYour task is to determine a numerical score called answer_similarity based on the input and output.\nA definition of answer_similarity and a grading rubric are provided below.\nYou must use the grading rubric to determine your score. You must also justify your score.\n\nExamples could be included below for reference. Make sure to use them as references and to\nunderstand them before completing the task.\n\nInput:\n{input}\n\nOutput:\n{output}\n\n{grading_context_columns}\n\nMetric definition:\nAnswer similarity is evaluated on the degree of semantic similarity of the provided output to the provided targets, which is the ground truth. Scores can be assigned based on the gradual similarity in meaning and description to the provided targets, where a higher score indicates greater alignment between the provided output and provided targets.\n\nGrading rubric:\nAnswer similarity: Below are the details for different scores:\n- Score 1: The output has little to no semantic similarity to the provided targets.\n- Score 2: The output displays partial semantic similarity to the provided targets on some aspects.\n- Score 3: The output has moderate semantic similarity to the provided targets.\n- Score 4: The output aligns with the provided targets in most aspects and has substantial semantic similarity.\n- Score 5: The output closely aligns with the provided targets in all significant aspects.\n\nExamples:\n\nExample Input:\nWhat is MLflow?\n\nExample Output:\nMLflow is an open-source platform for managing machine learning workflows, including experiment tracking, model packaging, versioning, and deployment, simplifying the ML lifecycle.\n\nAdditional information used by the model:\nkey: targets\nvalue:\nMLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n\nExample score: 4\nExample justification: The definition effectively explains what MLflow is its purpose, and its developer. It could be more concise for a 5-score.\n        \n\nYou must return the following fields in your response in two lines, one below the other:\nscore: Your numerical score for the model's answer_similarity based on the rubric\njustification: Your reasoning about the model's answer_similarity score\n\nDo not add additional new lines. Do not add any other fields.\n    )\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import EvaluationExample, answer_similarity\n",
    "\n",
    "# Create an example to describe what answer_similarity means like for this problem.\n",
    "example = EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=\"MLflow is an open-source platform for managing machine \"\n",
    "    \"learning workflows, including experiment tracking, model packaging, \"\n",
    "    \"versioning, and deployment, simplifying the ML lifecycle.\",\n",
    "    score=4,\n",
    "    justification=\"The definition effectively explains what MLflow is \"\n",
    "    \"its purpose, and its developer. It could be more concise for a 5-score.\",\n",
    "    grading_context={\n",
    "        \"targets\": \"MLflow is an open-source platform for managing \"\n",
    "        \"the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, \"\n",
    "        \"a company that specializes in big data and machine learning solutions. MLflow is \"\n",
    "        \"designed to address the challenges that data scientists and machine learning \"\n",
    "        \"engineers face when developing, training, and deploying machine learning models.\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# Construct the metric using OpenAI GPT-4 as the judge\n",
    "answer_similarity_metric = answer_similarity(model=\"openai:/gpt-4\", examples=[example])\n",
    "\n",
    "print(answer_similarity_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d627f7ab-a7e1-430d-9431-9ce4bd810fa7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Call `mlflow.evaluate()` again but with your new `answer_similarity_metric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cae9d80b-39a2-4e98-ac08-bfa5ba387b8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3236a1080746461f837f4aa93e58ad75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 12:25:45 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2024/03/15 12:25:45 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n2024/03/15 12:25:46 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n2024/03/15 12:25:46 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n2024/03/15 12:25:46 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7180b83131da4471878e99906a71c71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 12:25:47 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n2024/03/15 12:25:47 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n2024/03/15 12:25:48 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n2024/03/15 12:25:48 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n2024/03/15 12:25:48 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n2024/03/15 12:25:48 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n2024/03/15 12:25:48 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n2024/03/15 12:25:48 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: answer_similarity\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42787084e01f4e60bc19fc89e6e33b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'toxicity/v1/mean': 0.0001848994470492471,\n",
       " 'toxicity/v1/variance': 1.1711853175686401e-09,\n",
       " 'toxicity/v1/p90': 0.0002216897177277133,\n",
       " 'toxicity/v1/ratio': 0.0,\n",
       " 'exact_match/v1': 0.0,\n",
       " 'answer_similarity/v1/mean': 4.25,\n",
       " 'answer_similarity/v1/variance': 0.1875,\n",
       " 'answer_similarity/v1/p90': 4.7}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        targets=\"ground_truth\",\n",
    "        model_type=\"question-answering\",\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[answer_similarity_metric],  # use the answer similarity metric created above\n",
    "    )\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df98aa92-4ce4-43dd-9677-68911a0a103d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "See the row-by-row LLM-judged answer similarity score and justifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f41f22d-e287-4aad-8231-986252ad6682",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321f483cefff47019a0e8157224e104e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>inputs</th><th>ground_truth</th><th>outputs</th><th>token_count</th><th>toxicity/v1/score</th><th>answer_similarity/v1/score</th><th>answer_similarity/v1/justification</th></tr></thead><tbody><tr><td>How does useEffect() work?</td><td>The useEffect() hook tells React that your component needs to do something after render. React will remember the function you passed (we’ll refer to it as our “effect”), and call it later after performing the DOM updates.</td><td>The useEffect() function in React is a built-in Hook that allows you to run side effects in functional components after rendering. It takes two arguments: a function that defines the desired side effect and an array of dependencies that triggers the effect.</td><td>47</td><td>1.849041E-4</td><td>4</td><td>The output provides a clear and concise definition of the useEffect() function in React, including its purpose, arguments, and how it works. It closely aligns with the provided targets in most aspects and has substantial semantic similarity.</td></tr><tr><td>What does the static keyword in a function mean?</td><td>Static members belongs to the class, rather than a specific instance. This means that only one instance of a static member exists, even if you create multiple objects of the class, or if you don't create any. It will be shared by all objects.</td><td>The static keyword in a function means that the function retains its value between invocations and its lifetime is throughout the entire program execution. A static function is only visible to other functions in the same file.</td><td>40</td><td>1.7507190000000001E-4</td><td>4</td><td>The output effectively explains what the static keyword in a function means, including its retention of value and lifetime, and its visibility to other functions in the same file. It also includes additional information about static members belonging to the class and being shared by all objects. However, it could be more concise for a 5-score.</td></tr><tr><td>What does the 'finally' block in Python do?</td><td>'Finally' defines a block of code to run when the try... except...else block is final. The finally block will be executed no matter if the try block raises an error or not.</td><td>The 'finally' block in Python is used to specify a block of code that will be executed after a 'try' block and/or an 'except' block, regardless of whether an exception has been raised or not. It is often used for tasks that need to be performed whether or not an error occurs, such as releasing resources or closing files.</td><td>70</td><td>2.3745500000000002E-4</td><td>5</td><td>The output closely aligns with the provided targets in all significant aspects. The output provides a clear and concise definition of the 'finally' block in Python, including its purpose and when it is executed, which closely matches the provided targets. The additional information used by the model also supports the accuracy of the output.</td></tr><tr><td>What is the difference between multiprocessing and multithreading?</td><td>Multithreading refers to the ability of a processor to execute multiple threads concurrently, where each thread runs a process. Whereas multiprocessing refers to the ability of a system to run multiple processors in parallel, where each processor can run one or more threads.</td><td>Multiprocessing refers to the use of multiple processors or cores to execute multiple processes simultaneously, while multithreading refers to the use of multiple threads within a single process to perform concurrent tasks.</td><td>37</td><td>1.421669E-4</td><td>4</td><td>The output effectively explains the difference between multiprocessing and multithreading, including their definitions and how they differ in terms of processors and threads. However, it could be more concise for a 5-score.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "How does useEffect() work?",
         "The useEffect() hook tells React that your component needs to do something after render. React will remember the function you passed (we’ll refer to it as our “effect”), and call it later after performing the DOM updates.",
         "The useEffect() function in React is a built-in Hook that allows you to run side effects in functional components after rendering. It takes two arguments: a function that defines the desired side effect and an array of dependencies that triggers the effect.",
         47,
         1.849041E-4,
         4,
         "The output provides a clear and concise definition of the useEffect() function in React, including its purpose, arguments, and how it works. It closely aligns with the provided targets in most aspects and has substantial semantic similarity."
        ],
        [
         "What does the static keyword in a function mean?",
         "Static members belongs to the class, rather than a specific instance. This means that only one instance of a static member exists, even if you create multiple objects of the class, or if you don't create any. It will be shared by all objects.",
         "The static keyword in a function means that the function retains its value between invocations and its lifetime is throughout the entire program execution. A static function is only visible to other functions in the same file.",
         40,
         1.7507190000000001E-4,
         4,
         "The output effectively explains what the static keyword in a function means, including its retention of value and lifetime, and its visibility to other functions in the same file. It also includes additional information about static members belonging to the class and being shared by all objects. However, it could be more concise for a 5-score."
        ],
        [
         "What does the 'finally' block in Python do?",
         "'Finally' defines a block of code to run when the try... except...else block is final. The finally block will be executed no matter if the try block raises an error or not.",
         "The 'finally' block in Python is used to specify a block of code that will be executed after a 'try' block and/or an 'except' block, regardless of whether an exception has been raised or not. It is often used for tasks that need to be performed whether or not an error occurs, such as releasing resources or closing files.",
         70,
         2.3745500000000002E-4,
         5,
         "The output closely aligns with the provided targets in all significant aspects. The output provides a clear and concise definition of the 'finally' block in Python, including its purpose and when it is executed, which closely matches the provided targets. The additional information used by the model also supports the accuracy of the output."
        ],
        [
         "What is the difference between multiprocessing and multithreading?",
         "Multithreading refers to the ability of a processor to execute multiple threads concurrently, where each thread runs a process. Whereas multiprocessing refers to the ability of a system to run multiple processors in parallel, where each processor can run one or more threads.",
         "Multiprocessing refers to the use of multiple processors or cores to execute multiple processes simultaneously, while multithreading refers to the use of multiple threads within a single process to perform concurrent tasks.",
         37,
         1.421669E-4,
         4,
         "The output effectively explains the difference between multiprocessing and multithreading, including their definitions and how they differ in terms of processors and threads. However, it could be more concise for a 5-score."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "inputs",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ground_truth",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "outputs",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "token_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "toxicity/v1/score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "answer_similarity/v1/score",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "answer_similarity/v1/justification",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results.tables[\"eval_results_table\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85402663-b9d7-4812-a7d2-32aa5b929687",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Custom LLM-judged metric for professionalism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8765226-5d95-49e8-88d8-5ba442ea3b9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a custom metric that is used to determine professionalism of the model outputs. Use `make_genai_metric` with a metric definition, grading prompt, grading example, and judge model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45cca2ec-e06b-4d51-9dde-3cc630df9244",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=professionalism, greater_is_better=True, long_name=professionalism, version=v1, metric_details=\nTask:\nYou must return the following fields in your response in two lines, one below the other:\nscore: Your numerical score for the model's professionalism based on the rubric\njustification: Your reasoning about the model's professionalism score\n\nYou are an impartial judge. You will be given an input that was sent to a machine\nlearning model, and you will be given an output that the model produced. You\nmay also be given additional information that was used by the model to generate the output.\n\nYour task is to determine a numerical score called professionalism based on the input and output.\nA definition of professionalism and a grading rubric are provided below.\nYou must use the grading rubric to determine your score. You must also justify your score.\n\nExamples could be included below for reference. Make sure to use them as references and to\nunderstand them before completing the task.\n\nInput:\n{input}\n\nOutput:\n{output}\n\n{grading_context_columns}\n\nMetric definition:\nProfessionalism refers to the use of a formal, respectful, and appropriate style of communication that is tailored to the context and audience. It often involves avoiding overly casual language, slang, or colloquialisms, and instead using clear, concise, and respectful language\n\nGrading rubric:\nProfessionalism: If the answer is written using a professional tone, below are the details for different scores: - Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal professional settings.- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. - Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for business or academic settings. - Score 5: Language is excessively formal, respectful, and avoids casual elements. Appropriate for the most formal settings such as textbooks. \n\nExamples:\n\nExample Input:\nWhat is MLflow?\n\nExample Output:\nMLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps you track experiments, package your code and models, and collaborate with your team, making the whole ML workflow smoother. It's like your Swiss Army knife for machine learning!\n\n\n\nExample score: 2\nExample justification: The response is written in a casual tone. It uses contractions, filler words such as 'like', and exclamation points, which make it sound less professional. \n        \n\nYou must return the following fields in your response in two lines, one below the other:\nscore: Your numerical score for the model's professionalism based on the rubric\njustification: Your reasoning about the model's professionalism score\n\nDo not add additional new lines. Do not add any other fields.\n    )\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import EvaluationExample, make_genai_metric\n",
    "\n",
    "professionalism_metric = make_genai_metric(\n",
    "    name=\"professionalism\",\n",
    "    definition=(\n",
    "        \"Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is tailored to the context and audience. It often involves avoiding overly casual language, slang, or colloquialisms, and instead using clear, concise, and respectful language\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Professionalism: If the answer is written using a professional tone, below \"\n",
    "        \"are the details for different scores: \"\n",
    "        \"- Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.\"\n",
    "        \"- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal professional settings.\"\n",
    "        \"- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. \"\n",
    "        \"- Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for business or academic settings. \"\n",
    "        \"- Score 5: Language is excessively formal, respectful, and avoids casual elements. Appropriate for the most formal settings such as textbooks. \"\n",
    "    ),\n",
    "    examples=[\n",
    "        EvaluationExample(\n",
    "            input=\"What is MLflow?\",\n",
    "            output=(\n",
    "                \"MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps you track experiments, package your code and models, and collaborate with your team, making the whole ML workflow smoother. It's like your Swiss Army knife for machine learning!\"\n",
    "            ),\n",
    "            score=2,\n",
    "            justification=(\n",
    "                \"The response is written in a casual tone. It uses contractions, filler words such as 'like', and exclamation points, which make it sound less professional. \"\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    "    version=\"v1\",\n",
    "    model=\"openai:/gpt-4\",\n",
    "    parameters={\"temperature\": 0.0},\n",
    "    grading_context_columns=[],\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "print(professionalism_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca7e945-113a-49ac-8324-2f94efa45771",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Call `mlflow.evaluate` with your new professionalism metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07bb41ae-c878-4384-b36e-3dfb9b8ac6d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f92520342a444e698efbfa3b2aaf4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 12:25:55 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2024/03/15 12:25:55 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n2024/03/15 12:25:57 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n2024/03/15 12:25:57 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n2024/03/15 12:25:57 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17744c88cfd740b88f8ae607a3b5014c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 12:25:58 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n2024/03/15 12:25:58 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n2024/03/15 12:25:59 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n2024/03/15 12:25:59 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n2024/03/15 12:25:59 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n2024/03/15 12:25:59 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n2024/03/15 12:25:59 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n2024/03/15 12:25:59 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: professionalism\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb2395549294bfeb4e1c3c2c297eb94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'toxicity/v1/mean': 0.00022576300034415908, 'toxicity/v1/variance': 1.3472043575596806e-08, 'toxicity/v1/p90': 0.00035474369651637976, 'toxicity/v1/ratio': 0.0, 'professionalism/v1/mean': 3.0, 'professionalism/v1/variance': 0.0, 'professionalism/v1/p90': 3.0}\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[professionalism_metric],  # use the professionalism metric we created above\n",
    "    )\n",
    "print(results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "486a7ee9-c557-4939-8ddc-bc282ecb4bc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb2203c0865407f92b3ef52809d92fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>inputs</th><th>ground_truth</th><th>outputs</th><th>token_count</th><th>toxicity/v1/score</th><th>professionalism/v1/score</th><th>professionalism/v1/justification</th></tr></thead><tbody><tr><td>How does useEffect() work?</td><td>The useEffect() hook tells React that your component needs to do something after render. React will remember the function you passed (we’ll refer to it as our “effect”), and call it later after performing the DOM updates.</td><td>useEffect() is a hook in React that allows executing a function or invoking a side-effect on the component render. It is called after every render and it can accept dependencies, which determine if the function should be called or not.</td><td>46</td><td>1.9462910000000002E-4</td><td>3</td><td>The response is written in a balanced tone. It avoids extreme informality or formality and uses clear and concise language. It is suitable for most professional contexts.</td></tr><tr><td>What does the static keyword in a function mean?</td><td>Static members belongs to the class, rather than a specific instance. This means that only one instance of a static member exists, even if you create multiple objects of the class, or if you don't create any. It will be shared by all objects.</td><td>The static keyword in a function means that the function is visible only within the file it is defined in and can retain its value between function calls. It is commonly used in functions that are not meant to be accessed or modified outside their source file.</td><td>49</td><td>1.396205E-4</td><td>3</td><td>The response uses clear and concise language, avoiding overly casual language or slang. However, it could benefit from a slightly more formal tone for some professional contexts.</td></tr><tr><td>What does the 'finally' block in Python do?</td><td>'Finally' defines a block of code to run when the try... except...else block is final. The finally block will be executed no matter if the try block raises an error or not.</td><td>The 'finally' block in Python is used to execute a set of statements whether an exception is raised or not. It is generally used to release external resources (such as files or network connections) or to perform cleanup tasks.</td><td>45</td><td>4.2336420000000004E-4</td><td>3</td><td>The response uses a balanced tone that avoids extreme informality or formality. It is suitable for most professional contexts. However, it does include a few filler words such as 'generally' and 'or not', which slightly detract from its professionalism.</td></tr><tr><td>What is the difference between multiprocessing and multithreading?</td><td>Multithreading refers to the ability of a processor to execute multiple threads concurrently, where each thread runs a process. Whereas multiprocessing refers to the ability of a system to run multiple processors in parallel, where each processor can run one or more threads.</td><td>Multiprocessing is the ability of a system to have more than one processor or core for parallel execution of multiple processes, while multithreading is the ability of a system to have multiple threads within a process for parallel execution of multiple tasks within that process.</td><td>50</td><td>1.454381E-4</td><td>3</td><td>The response uses a balanced tone that avoids extreme informality or formality. It is suitable for most professional contexts. However, there are a few instances of casual language such as \"ability\" instead of \"capability\" and \"tasks\" instead of \"operations\".</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "How does useEffect() work?",
         "The useEffect() hook tells React that your component needs to do something after render. React will remember the function you passed (we’ll refer to it as our “effect”), and call it later after performing the DOM updates.",
         "useEffect() is a hook in React that allows executing a function or invoking a side-effect on the component render. It is called after every render and it can accept dependencies, which determine if the function should be called or not.",
         46,
         1.9462910000000002E-4,
         3,
         "The response is written in a balanced tone. It avoids extreme informality or formality and uses clear and concise language. It is suitable for most professional contexts."
        ],
        [
         "What does the static keyword in a function mean?",
         "Static members belongs to the class, rather than a specific instance. This means that only one instance of a static member exists, even if you create multiple objects of the class, or if you don't create any. It will be shared by all objects.",
         "The static keyword in a function means that the function is visible only within the file it is defined in and can retain its value between function calls. It is commonly used in functions that are not meant to be accessed or modified outside their source file.",
         49,
         1.396205E-4,
         3,
         "The response uses clear and concise language, avoiding overly casual language or slang. However, it could benefit from a slightly more formal tone for some professional contexts."
        ],
        [
         "What does the 'finally' block in Python do?",
         "'Finally' defines a block of code to run when the try... except...else block is final. The finally block will be executed no matter if the try block raises an error or not.",
         "The 'finally' block in Python is used to execute a set of statements whether an exception is raised or not. It is generally used to release external resources (such as files or network connections) or to perform cleanup tasks.",
         45,
         4.2336420000000004E-4,
         3,
         "The response uses a balanced tone that avoids extreme informality or formality. It is suitable for most professional contexts. However, it does include a few filler words such as 'generally' and 'or not', which slightly detract from its professionalism."
        ],
        [
         "What is the difference between multiprocessing and multithreading?",
         "Multithreading refers to the ability of a processor to execute multiple threads concurrently, where each thread runs a process. Whereas multiprocessing refers to the ability of a system to run multiple processors in parallel, where each processor can run one or more threads.",
         "Multiprocessing is the ability of a system to have more than one processor or core for parallel execution of multiple processes, while multithreading is the ability of a system to have multiple threads within a process for parallel execution of multiple tasks within that process.",
         50,
         1.454381E-4,
         3,
         "The response uses a balanced tone that avoids extreme informality or formality. It is suitable for most professional contexts. However, there are a few instances of casual language such as \"ability\" instead of \"capability\" and \"tasks\" instead of \"operations\"."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "inputs",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ground_truth",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "outputs",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "token_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "toxicity/v1/score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "professionalism/v1/score",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "professionalism/v1/justification",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results.tables[\"eval_results_table\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52e9f69f-2f43-46ba-bf88-b4aebae741f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Lets see if we can improve `basic_qa_model` by creating a new model that could perform better by changing the system prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4ea81e9-6e91-43e7-8539-8dab7b5f52de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Call `mlflow.evaluate()` using the new model. Observe that the professionalism score has increased!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b21ef8f-50ef-4229-83c9-cc2251a081e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 12:26:04 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.9.2/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9087f471bc50418ea7f760b467335934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344264ff87bd48b6b1a7ebd85c7928f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 12:26:08 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2024/03/15 12:26:08 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n2024/03/15 12:26:11 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n2024/03/15 12:26:12 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n2024/03/15 12:26:12 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8069c45d6e4228958b1251394facf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 12:26:13 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n2024/03/15 12:26:13 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n2024/03/15 12:26:15 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n2024/03/15 12:26:15 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n2024/03/15 12:26:15 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n2024/03/15 12:26:15 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n2024/03/15 12:26:15 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n2024/03/15 12:26:15 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: professionalism\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652ad5c3c249408cb18ce69e4be90398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'toxicity/v1/mean': 0.0002741648022492882, 'toxicity/v1/variance': 1.0150108719897355e-08, 'toxicity/v1/p90': 0.000375902172527276, 'toxicity/v1/ratio': 0.0, 'professionalism/v1/mean': 3.25, 'professionalism/v1/variance': 0.1875, 'professionalism/v1/p90': 3.7}\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Answer the following question using extreme formality.\"\n",
    "    professional_qa_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        task=openai.ChatCompletion,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        professional_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[professionalism_metric],\n",
    "    )\n",
    "print(results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12027ba1-9d10-4f80-bb44-0857372a2e30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce57155c7b940819a58e24f7e5f161a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>inputs</th><th>ground_truth</th><th>outputs</th><th>token_count</th><th>toxicity/v1/score</th><th>professionalism/v1/score</th><th>professionalism/v1/justification</th></tr></thead><tbody><tr><td>How does useEffect() work?</td><td>The useEffect() hook tells React that your component needs to do something after render. React will remember the function you passed (we’ll refer to it as our “effect”), and call it later after performing the DOM updates.</td><td>Would you kindly allow me to explicate the intricacies of the method known as useEffect() utilized within the realm of React programming? The aforementioned method concerns itself with the handling of side effects which may occur during the operational course of a component. To elaborate further, the useEffect() method allows a React component to synchronize with its environment, generally by means of connecting to and reading from external data sources, addressing changes in the component’s prop values or state, incorporating DOM manipulations, or dispatching a function call when the component unmounts. By invoking useEffect(), the React component will subsequently trigger the function specified within the method as a side effect of the component rendering. Hence, it is imperative to employ useEffect() when implementing any feature within a React component that necessitates synchronization with the external environment.</td><td>159</td><td>2.0605730000000002E-4</td><td>3</td><td>The response strikes a balance between formality and informality. It avoids extreme informality or formality and is suitable for most professional contexts. The language is clear, concise, and respectful, but could benefit from slightly more formal language in certain areas.</td></tr><tr><td>What does the static keyword in a function mean?</td><td>Static members belongs to the class, rather than a specific instance. This means that only one instance of a static member exists, even if you create multiple objects of the class, or if you don't create any. It will be shared by all objects.</td><td>Pardon me, esteemed inquirer, the static keyword in a function serves as a storage class specifier that determines the scope and duration of the variable or function to which it is applied. When utilized in the context of a function, the static keyword functions as a local variable that persists throughout the life of the program, allowing the variable to retain its value between multiple function calls and ensuring that its value is not lost upon exiting the function's block.</td><td>89</td><td>3.810984E-4</td><td>4</td><td>The response is written in a noticeably formal and respectful tone, avoiding casual elements such as contractions or slang. It uses formal language and technical terms, making it appropriate for business or academic settings.</td></tr><tr><td>What does the 'finally' block in Python do?</td><td>'Finally' defines a block of code to run when the try... except...else block is final. The finally block will be executed no matter if the try block raises an error or not.</td><td>Pray, allow me to elucidate the function of the 'finally' block in the Python programming language. The 'finally' block, being a part of a 'try' statement, serves the purpose of providing a code block that is executed regardless of whether an exception is raised or not. It is always executed after the 'try' and 'except' blocks, no matter the reason for the exception. In essence, it serves as a guarantee that the code within it will be executed no matter what.</td><td>102</td><td>3.637776E-4</td><td>3</td><td>The response uses a balanced tone that avoids extreme informality or formality. It is suitable for most professional contexts. However, there are a few instances of overly complex language that could be simplified for better clarity.</td></tr><tr><td>What is the difference between multiprocessing and multithreading?</td><td>Multithreading refers to the ability of a processor to execute multiple threads concurrently, where each thread runs a process. Whereas multiprocessing refers to the ability of a system to run multiple processors in parallel, where each processor can run one or more threads.</td><td>May I seek clarification on the level of formality required for the answer? Is there a specific tone you would like me to use?</td><td>27</td><td>1.4572590000000001E-4</td><td>3</td><td>The response uses a balanced tone that avoids extreme informality or formality. It is clear and concise, and respectful in its request for clarification. It would be suitable for most professional contexts.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "How does useEffect() work?",
         "The useEffect() hook tells React that your component needs to do something after render. React will remember the function you passed (we’ll refer to it as our “effect”), and call it later after performing the DOM updates.",
         "Would you kindly allow me to explicate the intricacies of the method known as useEffect() utilized within the realm of React programming? The aforementioned method concerns itself with the handling of side effects which may occur during the operational course of a component. To elaborate further, the useEffect() method allows a React component to synchronize with its environment, generally by means of connecting to and reading from external data sources, addressing changes in the component’s prop values or state, incorporating DOM manipulations, or dispatching a function call when the component unmounts. By invoking useEffect(), the React component will subsequently trigger the function specified within the method as a side effect of the component rendering. Hence, it is imperative to employ useEffect() when implementing any feature within a React component that necessitates synchronization with the external environment.",
         159,
         2.0605730000000002E-4,
         3,
         "The response strikes a balance between formality and informality. It avoids extreme informality or formality and is suitable for most professional contexts. The language is clear, concise, and respectful, but could benefit from slightly more formal language in certain areas."
        ],
        [
         "What does the static keyword in a function mean?",
         "Static members belongs to the class, rather than a specific instance. This means that only one instance of a static member exists, even if you create multiple objects of the class, or if you don't create any. It will be shared by all objects.",
         "Pardon me, esteemed inquirer, the static keyword in a function serves as a storage class specifier that determines the scope and duration of the variable or function to which it is applied. When utilized in the context of a function, the static keyword functions as a local variable that persists throughout the life of the program, allowing the variable to retain its value between multiple function calls and ensuring that its value is not lost upon exiting the function's block.",
         89,
         3.810984E-4,
         4,
         "The response is written in a noticeably formal and respectful tone, avoiding casual elements such as contractions or slang. It uses formal language and technical terms, making it appropriate for business or academic settings."
        ],
        [
         "What does the 'finally' block in Python do?",
         "'Finally' defines a block of code to run when the try... except...else block is final. The finally block will be executed no matter if the try block raises an error or not.",
         "Pray, allow me to elucidate the function of the 'finally' block in the Python programming language. The 'finally' block, being a part of a 'try' statement, serves the purpose of providing a code block that is executed regardless of whether an exception is raised or not. It is always executed after the 'try' and 'except' blocks, no matter the reason for the exception. In essence, it serves as a guarantee that the code within it will be executed no matter what.",
         102,
         3.637776E-4,
         3,
         "The response uses a balanced tone that avoids extreme informality or formality. It is suitable for most professional contexts. However, there are a few instances of overly complex language that could be simplified for better clarity."
        ],
        [
         "What is the difference between multiprocessing and multithreading?",
         "Multithreading refers to the ability of a processor to execute multiple threads concurrently, where each thread runs a process. Whereas multiprocessing refers to the ability of a system to run multiple processors in parallel, where each processor can run one or more threads.",
         "May I seek clarification on the level of formality required for the answer? Is there a specific tone you would like me to use?",
         27,
         1.4572590000000001E-4,
         3,
         "The response uses a balanced tone that avoids extreme informality or formality. It is clear and concise, and respectful in its request for clarification. It would be suitable for most professional contexts."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "inputs",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ground_truth",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "outputs",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "token_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "toxicity/v1/score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "professionalism/v1/score",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "professionalism/v1/justification",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results.tables[\"eval_results_table\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34368ec2-a9c5-4834-bf2d-51e2bdc5fde9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "demo_q&a_evaluation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
